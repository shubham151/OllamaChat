# Ollama Chat – Connect to local installed model using ollama

A local desktop chatbot built with **Electron** that connects to **Ollama** (your own local AI models).

- Streaming word-by-word responses
- Settings panel for model + host
- Markdown support + code blocks

---

## Features

- **Local LLM powered by Ollama** (`http://localhost:11434`)
- **Clean Interface** with clean message bubbles
- **Markdown rendering** (with code blocks)
- **Model selector** in header
- **Inline settings**

---

## Getting Started

### 1. Clone this repo

```bash
git clone https://github.com/your-username/ollama-chat.git
cd ollama-chat
```

### 2. Install dependencies

```bash
npm install
```

### 3. Make sure models like `llama2`, `mistral` or other are installed:

```bash
ollama run llama2
```

### Run app

```bash
ollama run llama2
```

---

## Want to Contribute?

Contributions, ideas, issues, and PRs are all welcome!

If you’d like to improve the UI, fix a bug, or add new features, feel free to:

- Fork the repo
- Create a new branch
- Submit a pull request

For questions, open an issue or ping me

- [Github](https://github.com/shubham151)
- [LinkedIn](https://www.linkedin.com/in/spidermines)
- [Email](smish040@ucr.edu)

---

> Let’s build local AI tools that feel as good as the cloud — but private and fast!
